# Real-Estate-House-Price-Prediction
[Application Landing Page (https://sites.google.com/njit.edu/house-price-predictor/home)](https://sites.google.com/njit.edu/house-price-predictor/home)
[Model Web Application (https://huggingface.co/spaces/saumyadwivedi/Real-Estate-House-Price-Prediction)](https://huggingface.co/spaces/saumyadwivedi/Real-Estate-House-Price-Prediction)
## Milestone 3
The work for Milestone 3 is showcased in Milestone3.ipynb and app.py. Dependencies for the application have been stored in requirements.txt while model.txt and oldmodel.txt are both the unoptimized and optimized models respectively. For more in-depth documentation, see the code comments. In Milestone3.ipynb, the a LightGBM model was constructed using the top 10 features with the highest mean absolute SHAP values for the sake of not overwhelming the application user. After uploading the data into the Colab environment, it was loaded into a Pandas dataframe where only the 10 features and the labels were kept. After this, the minimum and maximum values for the features were retrieved to restrict the slider movement on the application to prevent extrapolation. Then, Optuna was used to create a study to minimize model error using an objective function we defined. The optimal hyperparameters that resulted from this study were used to create and train a new model. After this, the model was saved to a text file called model.txt and deployed in a HuggingFace streamlit space. The code for this space is shown in app.py, and the application can be accessed via the link above. See how oldmodel.txt was saved in the description for Milestone 2 (it was also deployed on the web application). In app.py, the relevant libraries were imported, sliders were created and put into a list with labels that represented the features they were and minimum and maximum values that corresponded to those in the training data. Then, two functions were created, one to calculate the price with the optimized model and another to calculate the price with the unoptimized model. In each case, the function loaded and instantiated the model and iterated through the list of sliders, setting dictionary key-value pairs to the feature mapped to its value. This was stored into a dataframe, which was used to make a prediction with the model. After writing the predicted house price to the screen, the SHAP values and SHAP interaction values were calculated and graphed. Finally, two buttons were created, one that called the first function and another that called the second.
## Milestone 2
The work for Milestone 2 is showcased in Milestone2.ipynb. This is a higher level overview of the code as the comments provide an technical explanation of what is going on. First, the model training data was uploaded into the Colab environment, then loaded into a Pandas dataframe. Irrelevant features like "Id" were dropped, and the strategic choice to drop non-numeric features was made. Then, the data was separated into the data and labels for the sake of training. A simple LightGBM model, with an arbitrarily set learning rate, was constructed and fitted. Then, the importance of each feature was plotted based on its weight. After that, the SHAP values for all the features across all input data were calculated using the shap package. Using these, the bar plot of mean absolute SHAP values was plotted, showing the features that contributed the most (in either direction) in deviating the prediction from the mean. The top 3 features were shown to be ground living area in square footage, size of garage in terms of car capacity, and total basement square footage. Then, a beeswarm plot of the same SHAP values was created. This showed how the value of each feature impacted the model output. The top 3 features were the same as from the previous figure, but now we could see that a higher feature value for each of them resulted in a higher price prediction. An example of a feature in which this trend was reversed was unfinished square feet of basement area, a relatively low impact feature. Finally, the SHAP interaction values were plotted in a beeswarm plot, but no meaninful conclusions could be drawn from this graph. Note that after my initial Milestone 2 submission, I updated the notebook to also create, train, and download the model using the same 10 features from Milestone 3 as the optimized model, so that the output of both models could be compared on the application.
## Milestone 1
I already had WSL2 (for Intensive Linux Programming course) and VS Code IDE installed on my device prior to starting this project. To install Docker, I clicked on the link to bring me to a Docker Desktop download page. After downloading Docker Desktop installer, I waited for the appropriate packages to be installed by it and then restarted my device. I then created a Docker account and logged in on my Docker Desktop application after verifying my email. I tried following this tutorial (https://docs.docker.com/desktop/windows/wsl/) but everything was already properly configured on my machine. My Docker Desktop Application already utilized a WSL2 based engine and I already had Docker support enabled in my WSL 2 distro by default. I also had the WSL extension installed in VS Code already.

This is an image showing the Docker Desktop app installed on my device for use.
![image](https://user-images.githubusercontent.com/97859804/227812235-e889e41c-919c-42ae-9238-719b37af8863.png)
This is an image showing WSL2 running properly with a simple Docker command.
![image](https://user-images.githubusercontent.com/97859804/227812313-b6490f06-af3e-48dc-9f20-5ef38f395929.png)
This is an image of the IDE I will be using VS Code.
![image](https://user-images.githubusercontent.com/97859804/227812551-a4bbe6c9-928a-4cd2-8c71-7cec7c6deabd.png)
